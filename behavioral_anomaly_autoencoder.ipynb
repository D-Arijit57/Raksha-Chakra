{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d2acbf",
   "metadata": {},
   "source": [
    "# 🔍 Raksha Behavioral Anomaly Detection Model\n",
    "\n",
    "## Autoencoder-Based Fraud Detection for Mobile Banking\n",
    "\n",
    "This notebook trains a deep learning autoencoder model to detect anomalous behavioral patterns in mobile banking sessions. The model analyzes 30 behavioral biometric features to identify potentially fraudulent activity.\n",
    "\n",
    "### Features Analyzed:\n",
    "- **Touch Patterns**: Tap duration, swipe velocity, touch pressure, intervals\n",
    "- **Motion Sensors**: Accelerometer/gyroscope variance, device orientation  \n",
    "- **Device Context**: Battery, brightness, screen time, app usage\n",
    "- **Location/Network**: GPS coordinates, WiFi signatures\n",
    "- **Temporal Patterns**: Time of day, day of week encoding\n",
    "\n",
    "### Model Architecture:\n",
    "- **Type**: Autoencoder Neural Network\n",
    "- **Input**: 30 normalized features (18 continuous + 12 binary)\n",
    "- **Hidden Layers**: 32 → 16 → 32 neurons (ReLU activation)\n",
    "- **Output**: Reconstruction of input features\n",
    "- **Anomaly Detection**: Uses reconstruction error threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c326a370",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "First, let's install and import all necessary libraries for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cb208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow scikit-learn pandas numpy joblib matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b52a5",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Training Data\n",
    "\n",
    "Upload your `behavioral_training_data_6000_correlated.csv` file and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94989b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload your CSV file\n",
    "from google.colab import files\n",
    "print(\"📁 Please upload your behavioral_training_data_6000_correlated.csv file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Load the dataset\n",
    "file_name = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📊 Dataset shape: {df.shape}\")\n",
    "print(f\"📋 Columns: {len(df.columns)}\")\n",
    "print(f\"💾 Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n🔍 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18381861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset structure\n",
    "print(\"📊 Dataset Information:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "print(f\"Data types:\\n{df.dtypes.value_counts()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n🔍 Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_percent\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n📈 Dataset Statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac75a0c8",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Feature Engineering\n",
    "\n",
    "Define feature categories and prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd74fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature categories - Updated to match Flutter app columns (30 total features)\n",
    "continuous_features = [\n",
    "    \"tap_duration\", \"swipe_velocity\", \"touch_pressure\", \"tap_interval_avg\",\n",
    "    \"accel_variance\", \"gyro_variance\", \"battery_level\", \"brightness_level\",\n",
    "    \"screen_on_time\", \"time_of_day_sin\", \"time_of_day_cos\",\n",
    "    \"wifi_id_hash\", \"gps_latitude\", \"gps_longitude\",\n",
    "    \"device_orientation\", \"touch_area\", \"touch_event_count\", \"app_usage_time\"\n",
    "]\n",
    "\n",
    "binary_features = [\n",
    "    \"accel_variance_missing\", \"gyro_variance_missing\", \"charging_state\",\n",
    "    \"wifi_info_missing\", \"gps_location_missing\",\n",
    "    \"day_of_week_mon\", \"day_of_week_tue\", \"day_of_week_wed\",\n",
    "    \"day_of_week_thu\", \"day_of_week_fri\", \"day_of_week_sat\", \"day_of_week_sun\"\n",
    "]\n",
    "\n",
    "print(f\"📊 Feature Categories:\")\n",
    "print(f\"Continuous features: {len(continuous_features)}\")\n",
    "print(f\"Binary features: {len(binary_features)}\")\n",
    "print(f\"Total features: {len(continuous_features) + len(binary_features)}\")\n",
    "\n",
    "# Verify all features exist in dataset\n",
    "missing_features = []\n",
    "for feature in continuous_features + binary_features:\n",
    "    if feature not in df.columns:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"⚠️ Missing features in dataset: {missing_features}\")\n",
    "else:\n",
    "    print(\"✅ All features found in dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4c74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"🔧 Preprocessing data...\")\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Fill missing values with 0 (appropriate for behavioral data)\n",
    "df_processed[continuous_features] = df_processed[continuous_features].fillna(0)\n",
    "df_processed[binary_features] = df_processed[binary_features].fillna(0)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"Missing values after preprocessing: {df_processed[continuous_features + binary_features].isnull().sum().sum()}\")\n",
    "\n",
    "# Standardize continuous features\n",
    "print(\"📏 Scaling continuous features...\")\n",
    "scaler = StandardScaler()\n",
    "scaled_continuous = scaler.fit_transform(df_processed[continuous_features])\n",
    "\n",
    "# Combine scaled continuous and binary features\n",
    "X = np.hstack([scaled_continuous, df_processed[binary_features].values])\n",
    "\n",
    "print(f\"✅ Final feature matrix shape: {X.shape}\")\n",
    "print(f\"Feature matrix statistics:\")\n",
    "print(f\"  Min: {X.min():.4f}\")\n",
    "print(f\"  Max: {X.max():.4f}\")\n",
    "print(f\"  Mean: {X.mean():.4f}\")\n",
    "print(f\"  Std: {X.std():.4f}\")\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "print(f\"\\n📊 Data split:\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Testing set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbc676",
   "metadata": {},
   "source": [
    "## 4. Build Autoencoder Model Architecture\n",
    "\n",
    "Create the TensorFlow autoencoder model optimized for behavioral anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder architecture\n",
    "print(\"🏗️ Building autoencoder model...\")\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "# Define model architecture\n",
    "input_layer = Input(shape=(input_dim,), name='input')\n",
    "encoded = Dense(32, activation='relu', name='encoder_1')(input_layer)\n",
    "encoded = Dense(16, activation='relu', name='encoder_2')(encoded)\n",
    "decoded = Dense(32, activation='relu', name='decoder_1')(encoded)\n",
    "output_layer = Dense(input_dim, activation='linear', name='output')(decoded)\n",
    "\n",
    "# Create model\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer, name='behavioral_autoencoder')\n",
    "\n",
    "# Compile model\n",
    "autoencoder.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"✅ Model compiled successfully!\")\n",
    "print(\"\\n🏗️ Model Architecture:\")\n",
    "autoencoder.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(autoencoder, to_file='autoencoder_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "print(\"\\n📊 Model architecture saved as 'autoencoder_architecture.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a1c68",
   "metadata": {},
   "source": [
    "## 5. Train the Model\n",
    "\n",
    "Train the autoencoder on the behavioral data with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training parameters\n",
    "print(\"🚀 Starting model training...\")\n",
    "\n",
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"✅ Training completed!\")\n",
    "print(f\"Final epoch: {len(history.history['loss'])}\")\n",
    "print(f\"Best training loss: {min(history.history['loss']):.6f}\")\n",
    "print(f\"Best validation loss: {min(history.history['val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db25a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot training MAE\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE', color='blue')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE', color='red')\n",
    "plt.title('Model MAE During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display final metrics\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "print(f\"\\n📊 Final Training Metrics:\")\n",
    "print(f\"Training Loss: {final_train_loss:.6f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.6f}\")\n",
    "print(f\"Overfitting Check: {final_val_loss/final_train_loss:.2f}x training loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa45ee9",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Analyze reconstruction errors and determine anomaly detection threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c68bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "print(\"📊 Evaluating model performance...\")\n",
    "\n",
    "# Get predictions\n",
    "X_test_pred = autoencoder.predict(X_test, verbose=0)\n",
    "\n",
    "# Calculate reconstruction errors\n",
    "reconstruction_errors = np.mean(np.square(X_test - X_test_pred), axis=1)\n",
    "\n",
    "# Calculate statistics\n",
    "print(f\"📈 Reconstruction Error Statistics:\")\n",
    "print(f\"  Mean: {reconstruction_errors.mean():.6f}\")\n",
    "print(f\"  Std: {reconstruction_errors.std():.6f}\")\n",
    "print(f\"  Min: {reconstruction_errors.min():.6f}\")\n",
    "print(f\"  Max: {reconstruction_errors.max():.6f}\")\n",
    "\n",
    "# Calculate percentiles for threshold setting\n",
    "percentiles = [90, 95, 99]\n",
    "thresholds = {}\n",
    "for p in percentiles:\n",
    "    thresholds[p] = np.percentile(reconstruction_errors, p)\n",
    "    print(f\"  {p}th percentile: {thresholds[p]:.6f}\")\n",
    "\n",
    "# Use 95th percentile as anomaly threshold\n",
    "threshold = thresholds[95]\n",
    "print(f\"\\n🎯 Anomaly Detection Threshold (95th percentile): {threshold:.6f}\")\n",
    "\n",
    "# Create visualization of reconstruction errors\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Histogram of reconstruction errors\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(reconstruction_errors, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold (95th percentile)')\n",
    "plt.xlabel('Reconstruction Error')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Reconstruction Errors')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of reconstruction errors\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(reconstruction_errors, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "plt.ylabel('Reconstruction Error')\n",
    "plt.title('Box Plot of Reconstruction Errors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c793e7",
   "metadata": {},
   "source": [
    "## 7. Save Trained Model and Scaler\n",
    "\n",
    "Save the trained model and scaler for use in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "print(\"💾 Saving trained model and scaler...\")\n",
    "\n",
    "# Save the autoencoder model\n",
    "model_filename = 'autoencoder_model.h5'\n",
    "autoencoder.save(model_filename)\n",
    "print(f\"✅ Model saved as: {model_filename}\")\n",
    "\n",
    "# Save the scaler\n",
    "scaler_filename = 'scaler.pkl'\n",
    "joblib.dump(scaler, scaler_filename)\n",
    "print(f\"✅ Scaler saved as: {scaler_filename}\")\n",
    "\n",
    "# Save the threshold\n",
    "threshold_filename = 'threshold.pkl'\n",
    "joblib.dump(threshold, threshold_filename)\n",
    "print(f\"✅ Threshold saved as: {threshold_filename}\")\n",
    "\n",
    "# Save feature lists for reference\n",
    "feature_info = {\n",
    "    'continuous_features': continuous_features,\n",
    "    'binary_features': binary_features,\n",
    "    'total_features': len(continuous_features) + len(binary_features)\n",
    "}\n",
    "joblib.dump(feature_info, 'feature_info.pkl')\n",
    "print(f\"✅ Feature information saved as: feature_info.pkl\")\n",
    "\n",
    "# Download files to local machine\n",
    "print(\"\\n📥 Downloading files to your local machine...\")\n",
    "from google.colab import files\n",
    "\n",
    "files.download(model_filename)\n",
    "files.download(scaler_filename)\n",
    "files.download(threshold_filename)\n",
    "files.download('feature_info.pkl')\n",
    "\n",
    "print(\"\\n🎉 All files saved and downloaded successfully!\")\n",
    "print(\"\\nFiles ready for integration:\")\n",
    "print(f\"  • {model_filename} - Trained autoencoder model\")\n",
    "print(f\"  • {scaler_filename} - Feature scaler\")\n",
    "print(f\"  • {threshold_filename} - Anomaly threshold\")\n",
    "print(f\"  • feature_info.pkl - Feature definitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052eff0",
   "metadata": {},
   "source": [
    "## 8. Test Anomaly Detection Function\n",
    "\n",
    "Test the trained model with sample behavioral patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda6baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define anomaly detection function\n",
    "def score_session(session_dict, model, scaler, threshold):\n",
    "    \"\"\"\n",
    "    Score a single behavioral session for anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        session_dict: Dictionary containing all 30 behavioral features\n",
    "        model: Trained autoencoder model\n",
    "        scaler: Fitted StandardScaler\n",
    "        threshold: Anomaly detection threshold\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (reconstruction_error, is_anomaly, risk_score_percent)\n",
    "    \"\"\"\n",
    "    # Extract features in correct order\n",
    "    cont_vals = [session_dict[feat] for feat in continuous_features]\n",
    "    bin_vals = [session_dict[feat] for feat in binary_features]\n",
    "    \n",
    "    # Scale continuous features\n",
    "    scaled_cont = scaler.transform([cont_vals])\n",
    "    \n",
    "    # Combine all features\n",
    "    full_input = np.hstack([scaled_cont, [bin_vals]])\n",
    "    \n",
    "    # Get reconstruction\n",
    "    reconstructed = model.predict(full_input, verbose=0)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error = np.mean(np.square(full_input - reconstructed), axis=1)[0]\n",
    "    \n",
    "    # Determine if anomaly\n",
    "    is_anomaly = 1 if error > threshold else 0\n",
    "    \n",
    "    # Calculate risk score percentage\n",
    "    risk_score_percent = min(100, (error / threshold) * 100)\n",
    "    \n",
    "    return error, is_anomaly, risk_score_percent\n",
    "\n",
    "print(\"✅ Anomaly detection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c201a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample behavioral patterns\n",
    "print(\"🧪 Testing anomaly detection with sample patterns...\")\n",
    "\n",
    "# Anomalous behavioral pattern (suspicious)\n",
    "anomalous_example = {\n",
    "    'tap_duration': 0.09,                # Too fast – bot-like\n",
    "    'swipe_velocity': 0.2,               # Unusually slow swipe\n",
    "    'touch_pressure': 0.1,               # Very light touch\n",
    "    'tap_interval_avg': 0.05,            # Tapping too fast\n",
    "    'accel_variance': 0.95,              # Erratic movement\n",
    "    'gyro_variance': 0.95,               # High rotation — suspicious\n",
    "    'battery_level': 0.05,               # Very low battery\n",
    "    'brightness_level': 0.1,             # Screen barely visible\n",
    "    'screen_on_time': 0.95,              # Long screen on time – suspicious idle\n",
    "    'time_of_day_sin': -1.0,             # Edge of day (e.g., midnight)\n",
    "    'time_of_day_cos': 0.0,\n",
    "    'wifi_id_hash': 0.0,                 # Unrecognized network\n",
    "    'gps_latitude': 0.0,\n",
    "    'gps_longitude': 0.0,                # Unknown location\n",
    "    'device_orientation': 0.95,          # Unusual device orientation\n",
    "    'touch_area': 0.05,                  # Very small touch area - suspicious\n",
    "    'touch_event_count': 0.95,           # Too many touch events\n",
    "    'app_usage_time': 0.95,              # Suspiciously long app usage\n",
    "    'accel_variance_missing': 1,\n",
    "    'gyro_variance_missing': 1,\n",
    "    'charging_state': 0,                 # Not charging on low battery\n",
    "    'wifi_info_missing': 1,\n",
    "    'gps_location_missing': 1,\n",
    "    'day_of_week_mon': 0,\n",
    "    'day_of_week_tue': 0,\n",
    "    'day_of_week_wed': 0,\n",
    "    'day_of_week_thu': 0,\n",
    "    'day_of_week_fri': 0,\n",
    "    'day_of_week_sat': 0,\n",
    "    'day_of_week_sun': 1                 # Late night Sunday login (anomaly)\n",
    "}\n",
    "\n",
    "# Normal behavioral pattern\n",
    "normal_example = {\n",
    "    'tap_duration': 0.2,                 # Normal tap duration\n",
    "    'swipe_velocity': 0.75,              # Normal swipe speed\n",
    "    'touch_pressure': 0.7,               # Normal touch pressure\n",
    "    'tap_interval_avg': 0.3,             # Normal tap intervals\n",
    "    'accel_variance': 0.3,               # Normal movement\n",
    "    'gyro_variance': 0.25,               # Normal rotation\n",
    "    'battery_level': 0.6,                # Normal battery level\n",
    "    'brightness_level': 0.7,             # Normal brightness\n",
    "    'screen_on_time': 0.4,               # Normal screen time\n",
    "    'time_of_day_sin': 0.5,              # Normal time (afternoon)\n",
    "    'time_of_day_cos': 0.87,\n",
    "    'wifi_id_hash': 0.8,                 # Familiar network\n",
    "    'gps_latitude': 0.4,\n",
    "    'gps_longitude': 0.6,                # Normal location\n",
    "    'device_orientation': 0.5,           # Normal device orientation\n",
    "    'touch_area': 0.6,                   # Normal touch area\n",
    "    'touch_event_count': 0.4,            # Normal touch events\n",
    "    'app_usage_time': 0.3,               # Normal app usage time\n",
    "    'accel_variance_missing': 0,\n",
    "    'gyro_variance_missing': 0,\n",
    "    'charging_state': 1,                 # Device is charging\n",
    "    'wifi_info_missing': 0,\n",
    "    'gps_location_missing': 0,\n",
    "    'day_of_week_mon': 0,\n",
    "    'day_of_week_tue': 0,\n",
    "    'day_of_week_wed': 1,                # Wednesday (normal business day)\n",
    "    'day_of_week_thu': 0,\n",
    "    'day_of_week_fri': 0,\n",
    "    'day_of_week_sat': 0,\n",
    "    'day_of_week_sun': 0\n",
    "}\n",
    "\n",
    "# Test both examples\n",
    "print(\"\\\\n🔍 Testing Anomalous Pattern:\")\n",
    "error1, anomaly1, risk1 = score_session(anomalous_example, autoencoder, scaler, threshold)\n",
    "print(f\"  Reconstruction Error: {error1:.6f}\")\n",
    "print(f\"  Risk Score: {risk1:.1f}%\")\n",
    "print(f\"  Classification: {'⚠️ ANOMALY DETECTED' if anomaly1 else '✅ Normal'}\")\n",
    "\n",
    "print(\"\\\\n🔍 Testing Normal Pattern:\")\n",
    "error2, anomaly2, risk2 = score_session(normal_example, autoencoder, scaler, threshold)\n",
    "print(f\"  Reconstruction Error: {error2:.6f}\")\n",
    "print(f\"  Risk Score: {risk2:.1f}%\")\n",
    "print(f\"  Classification: {'⚠️ ANOMALY DETECTED' if anomaly2 else '✅ Normal'}\")\n",
    "\n",
    "print(f\"\\\\n📊 Model Performance Summary:\")\n",
    "print(f\"  Anomaly Threshold: {threshold:.6f}\")\n",
    "print(f\"  Anomalous pattern correctly flagged: {anomaly1 == 1}\")\n",
    "print(f\"  Normal pattern correctly flagged: {anomaly2 == 0}\")\n",
    "print(f\"  Model discrimination: {error1/error2:.1f}x higher error for anomaly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed89adab",
   "metadata": {},
   "source": [
    "## 🎉 Model Training Complete!\n",
    "\n",
    "### Summary\n",
    "Your behavioral anomaly detection model has been successfully trained and tested. The model can now identify suspicious behavioral patterns in mobile banking sessions.\n",
    "\n",
    "### Files Generated:\n",
    "- `autoencoder_model.h5` - Trained TensorFlow model\n",
    "- `scaler.pkl` - Feature preprocessing scaler\n",
    "- `threshold.pkl` - Anomaly detection threshold\n",
    "- `feature_info.pkl` - Feature definitions\n",
    "\n",
    "### Next Steps:\n",
    "1. **Download** all generated files to your local machine\n",
    "2. **Integrate** the model into your Flutter app's cloud ML service\n",
    "3. **Test** with real behavioral data from your mobile app\n",
    "4. **Monitor** model performance and retrain as needed\n",
    "\n",
    "### Model Performance:\n",
    "- **Input Features**: 30 behavioral biometric features\n",
    "- **Architecture**: 32→16→32 autoencoder with ReLU activation\n",
    "- **Threshold**: 95th percentile of reconstruction errors\n",
    "- **Detection**: Based on reconstruction error analysis\n",
    "\n",
    "The model is ready for production use in the Raksha mobile banking fraud detection system!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
